---
title: "Exploratory Data Analysis of Spotify Charts data"
author: "Daniel Winkler & Peter Knaus"
date: "2023"
output:
    pdf_document:
        toc: true
        toc_depth: 2
        theme: united
---


# EDA

## Read in Data and Setup Packages

First, load required packages.

```{r}
library(data.table)
library(dplyr)
library(ggraph)
library(igraph)
library(countrycode)
library(tidyr)
library(stringr)
library(nimble)
library(bayesplot)
```

Next up, we read in the data. We classify countries as neighboring if they (on average) share 1 song each half week in the top 10 songs on Spotify.

```{r}
el <- fread("./spotify_connections.csv.gz")
el[, streams_a_s := total_streams_a / max(total_streams_a)] # Normalized Streams
```

As the data contains 671 weeks of observations, 671/2 gives us minimum amount of shared songs in the top 10 for us to classify two countries as connected.

```{r}
el$weight_b <- ifelse(el$weight >= 671/2, 1, 0)
```

One region in the data is `global`, which we remove from the dataset. Further, we define some node properties that will be useful for plotting.

```{r}
# Remove global region and rename "gb" to "uk"
el <- el %>%
  filter(region_a != "global" & region_b != "global") %>%
  mutate(region_a = str_replace(region_a, "gb", "uk"),
         region_b = str_replace(region_b, "gb", "uk"))

# Use codelist dataframe from countrycode package 
# Remove "." from all names to match those in our dataset
codelist$region <- gsub("\\.", "", codelist$cctld)

# Grab relevant sublist from codelist 
sub_codelist <- codelist[, c("region", "iso3c", "continent", "country.name.en")]

# This creates the node properties
node_props <- el %>% 
  
  # We only need one row per node, so we group by and summarise 
  group_by(region_a) %>%
  summarise(total_streams = min(total_streams_a)) %>%
  
  # Some vertices only show up in region_a and some only in region_b, so we perform a full join to have both
  full_join(data.frame(region = sort(unique(el$region_b))), ., c("region" = "region_a")) %>%
  
  # Merge in the node properties from the codelist
  left_join(sub_codelist, by = "region") %>%
  
  # Replace the NA in argentina with an average (it never appears in the first column, so it has no total_streams_a)
  mutate(total_streams = replace_na(total_streams, mean(total_streams, na.rm = TRUE))) %>%
  
  # Scale down total streams
  mutate(total_streams = total_streams/sd(total_streams))
#extra_verts <- c(el$region_a, el$region_b) |> unique()
```

First visualization attempt:

```{r}
g <- graph_from_data_frame(el[el$weight_b == 1,], vertices = node_props, directed = FALSE)
plot(g)
```

Transform into adjacency matrix and plot.

```{r}
adj_mat <- as_adjacency_matrix(g)
A <- as.matrix(adj_mat)
pheatmap::pheatmap(A)
```

A plot of the eigen decomposition.

```{r}
# Take spectral decomposition of the unweighted adjacency matrix
eig.A <- eigen(A)

# Plot posterior clustering
evec.A <- eig.A$vectors

data.frame(V1 = evec.A[,1], V2 = evec.A[,2], Continent = as.factor(V(g)$continent), 
                 size = V(g)$total_streams, code = V(g)$iso3c) %>% 
  ggplot(aes(x = V1, y = V2, color = Continent, size = size, label = code), alpha = 0.15) + 
  #geom_jitter() + 
  geom_text(check_overlap = TRUE, show.legend = FALSE) + 
  theme_minimal() + 
  guides(size = "none") + 
  labs(title = "Data") + 
  scale_color_viridis_d()
```


A more advanced network plot.

```{r}
ggraph(g, layout = 'stress') + 
  geom_edge_link(alpha = 0.075) + # Set edge weight with attributes of g
  #scale_edge_width(range = c(0.5, 2.5)) +  # Constrain edge width
  geom_node_point(aes(size = total_streams, color = continent), alpha = 0.5) + #Set node color with attributes of g
  scale_size(range = c(2,10)) + # Constrain node sizes
  geom_node_text(aes(label = iso3c), repel = TRUE, point.padding = unit(0.5, "lines")) +
  theme_void() +
  scale_color_discrete(name = "Continent") + 
  scale_size_continuous(name = "SD of total streams")
  #theme(legend.position = "none")
```

Finally, some summary statistics.

```{r}
degree <- rowSums(A)
sort(degree)
```

```{r}
betweenness(g, v = V(g), directed = FALSE, weights = NA, normalized = TRUE)
```

```{r}
closeness(g, vids = V(g), weights = NA, normalized = TRUE)
```

```{r}
# Global clustering coefficient
C <- transitivity(g, type = "globalundirected")
C
```


```{r}
# Network average clustering coefficient
C.bar <- transitivity(g, type = "localaverageundirected")
C.bar
```

```{r}
sps <- shortest.paths(g, v = V(g), to = V(g), weights = NA)
mean(sps[upper.tri(sps)])
```
Seems to be pretty "small world"

```{r}
# Density, aka number of edges over total number of possible edges
edge_density(g)
```

```{r}
# Diameter, length of longest short path
diameter(g, directed = FALSE, weights = NA)
```


# First modeling attempts

First model only includes country specific random effects, as well as an indicator if the countries lie on the same continent. (Here, the only covariate $x_i$ determines the continent an observation is on.)

$$
h(\pi_{ij}) = \alpha_i + \alpha_j + \beta \mathbb 1(x_i = x_j)
$$

First, define necessary data and constatnts.

```{r}
nC <- nrow(adj_mat)

# Define the model constants
glmConsts <- list(N = nC)

# Define the model data
cont <- as.factor(node_props$continent) 
glmData <- list(
  y = A,
  x = outer(cont, cont, FUN = "==")*1 # x_ij = 1(region_i == region_j)
)

# Define the initial values
glmInits <- list(alpha = rep(0, nC), beta1 = 0, sigma = 1, p = matrix(0, nC, nC))

# Define the dimensions
glmDims <- list(p = c(nC, nC))
```

Next, define the model in NIMBLE notation.

```{r}
glmCode <- nimbleCode({
  # Priors
  #beta0 ~ dnorm(0, sd = 1) # some use sd = 10000!
  beta1 ~ dnorm(0, sd = 1)
  
  for (k in 1:N) {
    alpha[k] ~ dnorm(0, sd = sigma)
  }
  
  sigma ~ dunif(0, 10)
  
  # Likelihood
  for (i in 2:N) { 
    for (j in 1:(i-1)){
      logit(p[i,j]) <- alpha[i] + alpha[j] + beta1 * x[i,j] #beta0 
      p[j,i] <- p[i,j]
      y[i,j] ~ dbin(size = 1, prob = p[i,j])
    }
  }
  
  for(i in 1:nC){ # clunky code to avoid NA in p matrix
    p[i,i] <- 0
  }
  
})

```

Define NIMBLE objects.

```{r}
glmModel <- nimbleModel(code = glmCode, constants = glmConsts, data = glmData, 
                        inits = glmInits)

configureMCMC(glmModel, print = TRUE, onlySlice = TRUE)

set.seed(1234)
niter <- 20000
nburnin <- 2000
nchains <- 2
mcmc.out <- nimbleMCMC(glmModel,
                       nchains = nchains, niter = niter, nburnin = nburnin,
                       summary = TRUE, WAIC = TRUE,
                       monitors = c('alpha', 'beta1', 'p'), 
                       samplesAsCodaMCMC = TRUE) # Use this option if you plan on using coda
```

Extract samples

```{r}
# Extract samples 
samples <- mcmc.out$samples # as list of chains
all.samples <- do.call(rbind, mcmc.out$samples) # as matrix

# Extract posterior samples of the probability matrix
p.samples <- all.samples[, grepl("^p", colnames(all.samples))]
p.post <- matrix(data = colMeans(p.samples), byrow = FALSE, nrow = nC, ncol = nC)
```


Trace plot of beta

```{r}
mcmc_trace(samples, regex_pars = c("beta"))
```

```{r}
obsA <-  A[upper.tri(A)]
df <- data.frame(pred = p.post[upper.tri(p.post)], data = obsA)
pROC::plot.roc(df$data ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Baseline GLM")
```

Posterior predictive checks:

```{r}
A.r <- matrix(NA, nrow = nC, ncol = nC)
postD <- matrix(NA, nrow = nrow(p.samples), ncol = 1)
allD <- matrix(NA, nrow = nrow(p.samples), ncol = nC)
for(r in 1:nrow(p.samples)){
  
  # Sample adjacency matrix
  p.post.r <- matrix(data = p.samples[r,], byrow = FALSE, nrow = nC, ncol = nC)
  diag(p.post.r) <- 0
  A.r[upper.tri(A.r)] <- rbinom(nC*(nC - 1)/2, 1, p.post.r[upper.tri(p.post.r)])
  A.r[lower.tri(A.r)] <- t(A.r)[lower.tri(A.r)]
  diag(A.r) <- 0
  
  # Sample network density
  postD[r,] <- sum(A.r[lower.tri(A.r)])/(nC*(nC -1)/2)
  allD[r,] <- rowSums(A.r)
}

D.obs <- sum(A[lower.tri(A)])/(nC*(nC -1)/2)
hist(postD)
abline(v = D.obs)

```

```{r}
all.D.obs <- rowSums(A)
pp_check(all.D.obs, allD[2000:2050, ], ppc_dens_overlay)
```

# Creating SBM model for our data

Some dimension notes: We have $N$ individuals and $K$ clusters. $Z$ is $N\times K$, with $Z_i$ being the $i$th row. $\Theta$ is $K\times K$.  

$$
Y_{ij}|Z,\Theta \sim \text{Bern}\left(Z_i\Theta Z_j'\right) \\
\theta_{pq}\sim \text{Beta}(a^{o}, b^o), \text{ for } p \neq q \\
\theta_{pp}\sim \text{Beta}(a^{c}, b^c)\\
Z_{i} \sim \text{Cat}(\lambda) \\
\lambda \sim \text{Dir}(\boldsymbol \alpha)
$$

NIMBLE code for SBM:

```{r}
# Define model with BUGS code
sbmCode <- nimbleCode({
  
  # lambda - latent group assignment probabilities (vector of length K)
  lambda[1:K] ~ ddirch(alpha[]) 
  
  # Z - latent group indicator (binary vector of length K, summing to 1)
  for(i in 1:N){
    Z[i] ~ dcat(prob = lambda[])
  }
    # theta - symmetric matrix of within and between group edge probabilities
  for (i in 1:K){
    theta[i,i] ~ dbeta(shape1 = a_c, shape2 = b_c) # Within block connections
    for (j in 1:(i-1)){ 
      theta[i,j] ~ dbeta(shape1 = a_o, shape2 = b_o) # Between block connections 
      theta[j,i] <- theta[i,j] # symmetric matrix
    }
  }

  # Pi - node to node edge probabilities (based on group membership)
  for (i in 2:N){
    for (j in 1:(i-1)){ # Self-edges not allowed
      Pi[i,j] <- myCalculation(theta[,], Z[i], Z[j]) # Workaround because nimble does not allow indexing by latent variables
      Y[i,j] ~ dbin(size = 1, prob = Pi[i,j])
    }
  }

})

## User-defined functions: written in NIMBLE
myCalculation <- nimbleFunction(
  run = function(grid = double(2), index1 = double(0), index2 = double(0)) {  ## index could be int() but model variables are represented as double anyway
    return(grid[index1, index2])
    returnType(double(0))
  })

```

Define constants, inits and data for SBM

```{r}

K <- 10
N <- nrow(A)
a_o <- .5
b_o <- 1
a_c <- 1
b_c <- .5

alpha <- 0.01#1/K
 
# Define the constants
sbmConsts <- list(N = N, K= K,
                  a_o = a_o, b_o = b_o,
                  a_c = a_c, b_c = b_c,
                  alpha =  rep(alpha, K))

# Define the data
sbmData <- list(Y = A)

theta_init <- matrix(.2, K, K)
diag(theta_init) <- 0.6

sbmInits <- list(lambda = rep(1/K, K), # block assignment probs
                 theta = theta_init, # edge probs, better init
                 Pi = matrix(mean(A[lower.tri(A)]), N, N), # edge probs
                 Z = sample(1:K, N, TRUE)) # group indicators
```

TODO: Add loglik to output to monitor overall mixing

```{r}
# Easiest way to run: 
niter <- 50000
nburn <- 10000
nthin <- 10
nchains <- 2
mcmc.out <- nimbleMCMC(code = sbmCode, constants = sbmConsts,
                       data = sbmData, inits = sbmInits,
                       nchains = nchains, niter = niter, nburnin = nburn, thin = nthin,
                       summary = TRUE, WAIC = TRUE,
                       monitors = c('lambda', 'theta', 'Z'))
```

Take a first look at results!

```{r}
samples <- mcmc.out$samples
mcmc_trace(samples, regex_pars = c("lambda"))
```

```{r}
# Extract mcmc  samples
samples <- do.call(rbind, mcmc.out$samples)
z.samples <- samples[, grepl("Z", colnames(samples))]

# Compute posterior similarity matrix
z.psm <- comp.psm(z.samples) 
plotpsm(z.psm) # note data points are reordered by hierarchical clustering here

# Find a representative partition of posterior by minimizing VI
z.vi <- minVI(z.psm, method = "greedy") 
summary(z.vi) # if you use method = "all", this compares them all 
z.cl.post <- z.vi$cl
```

```{r}
eig.A <- eigen(A)

# Plot posterior clustering
evec.A <- eig.A$vectors

conts <- V(g)$continent

data.frame(eigen1 = rep(evec.A[,1], 2), 
           eigen2 = rep(evec.A[,2], 2),
           group = as.factor(c(as.numeric(as.factor(conts)), z.cl.post)),
           grouping = as.factor(rep(c("continents", "estim"), each = N)),
           label = rep(V(g)$iso3c, 2)) %>% 
  ggplot(aes(x = eigen1, y = eigen2, color = group, label = label), alpha = 0.15) + 
  geom_text(check_overlap = T, show.legend = F) +
  facet_grid(~grouping) +
  #geom_jitter() + 
  theme_minimal() + 
  guides(size = "none") + 
  labs(title = "Data") + 
  scale_color_brewer(type = "qual", palette = "Set1")
```

```{r}
ggraph(g, layout = 'stress') + 
  geom_edge_link(alpha = 0.075) + # Set edge weight with attributes of g
  #scale_edge_width(range = c(0.5, 2.5)) +  # Constrain edge width
  geom_node_point(aes(size = degree(g)/N, color = as.factor(z.cl.post)), alpha = 0.5) + #Set node color with attributes of g
  scale_size(range = c(2,10)) + # Constrain node sizes
  geom_node_text(aes(label = iso3c), repel = TRUE, point.padding = unit(0.5, "lines")) +
  theme_void() +
  scale_color_discrete(name = "Estimated Group") + 
  scale_size_continuous(name = "Normalized degree")
  #theme(legend.position = "none")
```

# Factor Model

```{r}
# Define the constants
diag(A) <- 0 # diag = NA is useful for plotting only
mu0 = log(mean(A)/(1-mean(A))) # prior mean for Mu is logit(meanY)
H <- 10 # n factors
a1 <- 2.5 # shrinkage first factor
a2 <- 2 # shrinkage other factors
sd.mu <- 0.1
fmConsts <- list(V = N,
                 H = H, 
                 a1 = a1, a2 = a2,
                 mu0 = mu0, sd.mu = sd.mu,
                 M = rep(0, N))

# Define the data
fmData <- list(Y = A)

# Set initialization parameters
fmInits <- list(X = matrix(0, N, H), 
                U = 1:H,
                mu = mu0,
                )

fmDims <- list(Tau = H, X = c(N, H), P = c(N,N))

```


```{r}
# Define model with BUGS code
fmCode <- nimbleCode({
  
  # Intercept
  mu ~ dnorm(mu0, sd = sd.mu)
  
  # Shrinkage process
  U[1] ~ dgamma(a1, 1)
  for(h in 2:H){
    U[h] ~ dgamma(a2, 1)
  }

  for(h in 1:H){
    Tau[h] <- prod(U[1:h])
  }

  # Latent factors
  for(h in 1:H){
    for(v in 1:V){
      X[v,h] ~ dnorm(M[v] , sd = sqrt(Tau[h]^(-1)))
    }
  }
  
  # Compute linear predictor
  M.Z[1:V,1:V] <- mu + X[,] %*% t(X[,]) # Recall multivariate nodes must be used with []

  # Likelihood
  for (i in 2:V){
    for (j in 1:(i-1)){ # Self-edges not allowed
      P[i,j] <- phi(M.Z[i,j]) # Gaussian CDF 
      P[j,i] <- P[i,j]
      Y[i,j] ~ dbin(size = 1, prob = P[i,j])
    }
  }

})



```

```{r}

nchains <- 2
niter <- 5000
nburnin <- 2000

samplesHMC <- nimbleHMC(fmCode, data = fmData, inits = fmInits, constants = fmConsts, 
          monitors = c("P", "mu", "Tau"), WAIC = TRUE, 
          niter = niter, nburnin = nburnin, nchains = nchains)
```
```{r}
all.samples <- do.call(rbind, samplesHMC$samples) # create dataframe with all chains
all.samples[is.na(all.samples)] <- 0
#all.samples <- samplesHMC$samples
p.HMC <- all.samples[, grepl("P", colnames(all.samples))]
# p.HMC
# Latent probability recovery: only possible with simulated data
p.HMC.means <- colMeans(p.HMC)
p.post.HMC <- matrix(p.HMC.means, byrow = FALSE, nrow = N, ncol = N)
p.post.HMC
diag(p.post.HMC) <- 0

df <- data.frame(p.post = p.post.HMC[upper.tri(p.post.HMC)],  y = A[upper.tri(A)] )
plot.roc(df$y ~ df$p.post, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Factor Model with Simulated Data")

mcmc_trace(all.samples, regex_pars = "Tau")
```

