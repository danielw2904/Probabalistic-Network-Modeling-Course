---
title: "Exploratory Data Analysis of Spotify Charts data"
author: "Daniel Winkler & Peter Knaus"
date: "2023"
output:
    pdf_document:
        toc: true
        toc_depth: 2
        theme: united
---


# EDA

## Read in Data and Setup Packages

First, load required packages.

```{r}
library(data.table)
library(dplyr)
library(ggraph)
library(igraph)
library(countrycode)
library(tidyr)
library(stringr)
library(nimble)
library(bayesplot)
```

Next up, we read in the data. We classify countries as neighboring if they (on average) share 1 song each half week in the top 10 songs on Spotify.

```{r}
el <- fread("./spotify_connections.csv.gz")
el[, streams_a_s := total_streams_a / max(total_streams_a)] # Normalized Streams
```

As the data contains 671 weeks of observations, 671/2 gives us minimum amount of shared songs in the top 10 for us to classify two countries as connected.

```{r}
el$weight_b <- ifelse(el$weight >= 671/2, 1, 0)
```

One region in the data is `global`, which we remove from the dataset. Further, we define some node properties that will be useful for plotting.

```{r}
# Remove global region and rename "gb" to "uk"
el <- el %>%
  filter(region_a != "global" & region_b != "global") %>%
  mutate(region_a = str_replace(region_a, "gb", "uk"),
         region_b = str_replace(region_b, "gb", "uk"))

# Use codelist dataframe from countrycode package 
# Remove "." from all names to match those in our dataset
codelist$region <- gsub("\\.", "", codelist$cctld)

# Grab relevant sublist from codelist 
sub_codelist <- codelist[, c("region", "iso3c", "continent", "country.name.en")]

# This creates the node properties
node_props <- el %>% 
  
  # We only need one row per node, so we group by and summarise 
  group_by(region_a) %>%
  summarise(total_streams = min(total_streams_a)) %>%
  
  # Some vertices only show up in region_a and some only in region_b, so we perform a full join to have both
  full_join(data.frame(region = sort(unique(el$region_b))), ., c("region" = "region_a")) %>%
  
  # Merge in the node properties from the codelist
  left_join(sub_codelist, by = "region") %>%
  
  # Replace the NA in argentina with an average (it never appears in the first column, so it has no total_streams_a)
  mutate(total_streams = replace_na(total_streams, mean(total_streams, na.rm = TRUE))) %>%
  
  # Scale down total streams
  mutate(total_streams = total_streams/sd(total_streams))
#extra_verts <- c(el$region_a, el$region_b) |> unique()
```

First visualization attempt:

```{r}
g <- graph_from_data_frame(el[el$weight_b == 1,], vertices = node_props, directed = FALSE)
plot(g)
```

Transform into adjacency matrix and plot.

```{r}
adj_mat <- as_adjacency_matrix(g)
A <- as.matrix(adj_mat)
pheatmap::pheatmap(A)
```

A plot of the eigen decomposition.

```{r}
# Take spectral decomposition of the unweighted adjacency matrix
eig.A <- eigen(A)

# Plot posterior clustering
evec.A <- eig.A$vectors

data.frame(V1 = evec.A[,1], V2 = evec.A[,2], Continent = as.factor(V(g)$continent), 
                 size = V(g)$total_streams, code = V(g)$iso3c) %>% 
  ggplot(aes(x = V1, y = V2, color = Continent, size = size, label = code), alpha = 0.15) + 
  #geom_jitter() + 
  geom_text(check_overlap = TRUE, show.legend = FALSE) + 
  theme_minimal() + 
  guides(size = "none") + 
  labs(title = "Data") + 
  scale_color_viridis_d()
```


A more advanced network plot.

```{r}
ggraph(g, layout = 'stress') + 
  geom_edge_link(alpha = 0.075) + # Set edge weight with attributes of g
  #scale_edge_width(range = c(0.5, 2.5)) +  # Constrain edge width
  geom_node_point(aes(size = total_streams, color = continent), alpha = 0.5) + #Set node color with attributes of g
  scale_size(range = c(2,10)) + # Constrain node sizes
  geom_node_text(aes(label = iso3c), repel = TRUE, point.padding = unit(0.5, "lines")) +
  theme_void() +
  scale_color_discrete(name = "Continent") + 
  scale_size_continuous(name = "SD of total streams")
  #theme(legend.position = "none")
```

Finally, some summary statistics.

```{r}
degree <- rowSums(A)
sort(degree)
```

```{r}
betweenness(g, v = V(g), directed = FALSE, weights = NA, normalized = TRUE)
```

```{r}
closeness(g, vids = V(g), weights = NA, normalized = TRUE)
```

```{r}
# Global clustering coefficient
C <- transitivity(g, type = "globalundirected")
C
```


```{r}
# Network average clustering coefficient
C.bar <- transitivity(g, type = "localaverageundirected")
C.bar
```

```{r}
sps <- shortest.paths(g, v = V(g), to = V(g), weights = NA)
mean(sps[upper.tri(sps)])
```
Seems to be pretty "small world"

```{r}
# Density, aka number of edges over total number of possible edges
edge_density(g)
```

```{r}
# Diameter, length of longest short path
diameter(g, directed = FALSE, weights = NA)
```


# First modeling attempts

First model only includes country specific random effects, as well as an indicator if the countries lie on the same continent. (Here, the only covariate $x_i$ determines the continent an observation is on.)

$$
h(\pi_{ij}) = \alpha_i + \alpha_j + \beta \mathbb 1(x_i = x_j)
$$

First, define necessary data and constatnts.

```{r}
nC <- nrow(adj_mat)

# Define the model constants
glmConsts <- list(N = nC)

# Define the model data
cont <- as.factor(node_props$continent) 
glmData <- list(
  y = A,
  x = outer(cont, cont, FUN = "==")*1 # x_ij = 1(region_i == region_j)
)

# Define the initial values
glmInits <- list(alpha = rep(0, nC), beta1 = 0, sigma = 1, p = matrix(0, nC, nC))

# Define the dimensions
glmDims <- list(p = c(nC, nC))
```

Next, define the model in NIMBLE notation.

```{r}
glmCode <- nimbleCode({
  # Priors
  #beta0 ~ dnorm(0, sd = 1) # some use sd = 10000!
  beta1 ~ dnorm(0, sd = 1)
  
  for (k in 1:N) {
    alpha[k] ~ dnorm(0, sd = sigma)
  }
  
  sigma ~ dunif(0, 10)
  
  # Likelihood
  for (i in 2:N) { 
    for (j in 1:(i-1)){
      logit(p[i,j]) <- alpha[i] + alpha[j] + beta1 * x[i,j] #beta0 
      p[j,i] <- p[i,j]
      y[i,j] ~ dbin(size = 1, prob = p[i,j])
    }
  }
  
  for(i in 1:nC){ # clunky code to avoid NA in p matrix
    p[i,i] <- 0
  }
  
})

```

Define NIMBLE objects.

```{r}
glmModel <- nimbleModel(code = glmCode, constants = glmConsts, data = glmData, 
                        inits = glmInits)

configureMCMC(glmModel, print = TRUE, onlySlice = TRUE)

set.seed(1234)
niter <- 20000
nburnin <- 2000
nchains <- 2
mcmc.out <- nimbleMCMC(glmModel,
                       nchains = nchains, niter = niter, nburnin = nburnin,
                       summary = TRUE, WAIC = TRUE,
                       monitors = c('alpha', 'beta1', 'p'), 
                       samplesAsCodaMCMC = TRUE) # Use this option if you plan on using coda
```

Extract samples

```{r}
# Extract samples 
samples <- mcmc.out$samples # as list of chains
all.samples <- do.call(rbind, mcmc.out$samples) # as matrix

# Extract posterior samples of the probability matrix
p.samples <- all.samples[, grepl("^p", colnames(all.samples))]
p.post <- matrix(data = colMeans(p.samples), byrow = FALSE, nrow = nC, ncol = nC)
```


Trace plot of beta

```{r}
mcmc_trace(samples, regex_pars = c("beta"))
```

```{r}
obsA <-  A[upper.tri(A)]
df <- data.frame(pred = p.post[upper.tri(p.post)], data = obsA)
pROC::plot.roc(df$data ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Baseline GLM")
```

Posterior predictive checks:

```{r}
A.r <- matrix(NA, nrow = nC, ncol = nC)
postD <- matrix(NA, nrow = nrow(p.samples), ncol = 1)
allD <- matrix(NA, nrow = nrow(p.samples), ncol = nC)
for(r in 1:nrow(p.samples)){
  
  # Sample adjacency matrix
  p.post.r <- matrix(data = p.samples[r,], byrow = FALSE, nrow = nC, ncol = nC)
  diag(p.post.r) <- 0
  A.r[upper.tri(A.r)] <- rbinom(nC*(nC - 1)/2, 1, p.post.r[upper.tri(p.post.r)])
  A.r[lower.tri(A.r)] <- t(A.r)[lower.tri(A.r)]
  diag(A.r) <- 0
  
  # Sample network density
  postD[r,] <- sum(A.r[lower.tri(A.r)])/(nC*(nC -1)/2)
  allD[r,] <- rowSums(A.r)
}

D.obs <- sum(A[lower.tri(A)])/(nC*(nC -1)/2)
hist(postD)
abline(v = D.obs)

```

```{r}
all.D.obs <- rowSums(A)
pp_check(all.D.obs, allD[2000:2050, ], ppc_dens_overlay)
```

