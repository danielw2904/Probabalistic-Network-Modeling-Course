---
title: "Latent Factors"
output: html_document
date: "2023-10-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# TO DO:

- Experiment with more nodes vs more time periods and see what works


```{r, echo = FALSE, message = FALSE}
## Set directories
data_path <- 'Data/'
save_path <- 'Results/'

# Load packages
library(tidyverse)
library(here)
library(igraph)
library(ggraph)
library(RColorBrewer)
library(pheatmap)
library(nimble)
library(nimbleHMC)
library(bayesplot)
library(mvtnorm)
library(gridExtra)

# Do we want to save any output?
save_files <- TRUE

```
# PART 1: SIMULATE FROM A LATENT SPACE MODEL 

First, we simulate data from an inner product graph model. Let's begin with only
10 nodes for ease of visualization

## 1a. Visualizing a small network 
Here we simulate from a latent space network model. Note that although there is no
data augmentation $Z$ in the code below, however it is equivalent the the Albert and Chib (1993)
method presented in lecture. 
```{r}
set.seed(1234)

# Data (and latent space) dimensions
V <- 10 # Number of vertices
H <- 10 # Dimension of latent space
Times <- sort(sample(1:50, 10)) # Vector of points in time, worked with 10 times but not 20 for some reason
R <- length(Times)
V.names <- paste0("Node ", 1:V)
T.names <- paste0("Time ", Times)

# Hyperparameters
a1 <- 2.5 # Gamma shrinkage parameter for factor 1
a2 <- 2.5 # Gamma shrinkage parameters for factors 2:H
K_x <- K_mu <- 0.01 # GP covar parameters

meanP <- 0.25 # Moderately sparse network
mu0 <- probit(meanP) # Prior mean for intercept
sd.mu <- 0.1 # Prior sd for intercept: consider probit scale INCORPORATE THIS INTO KMU

# Simulate multiplicative gamma shrinkage process
U <- rep(NA,H)
U[1] <- rgamma(1, a1, 1)
U[2:H] <- rgamma(H-1, a2, 1)

Tau <- rep(NA, H)
for(h in 1:H){
  Tau[h] <- prod(U[1:h])
}

### Simulate latent factors

# X(t) process covariance
D <- as.matrix(dist(Times, diag = TRUE, upper = TRUE), nrow = R)
C_X <- exp(-K_x*D^2)

# Look at induced covariance 
rownames(C_X) <- colnames(C_X) <- Times
pheatmap(C_X, cluster_rows = FALSE, cluster_cols = FALSE)

# X storage
X <- array(NA, dim = c(V,H,R))
X <- provideDimnames(X , sep = "_", base = list("Node", "Factor", "Time"))

# Simulate from GP
for(h in 1:H){
  for(v in 1:V){
      X[v,h,] <- rmvnorm(n = 1, mean = rep(0, R), sigma = Tau[h]^(-1)*C_X) 
    }
  }


# Simulate intercept
C_mu <- exp(-K_mu*D^2)
mu <- matrix(rmvnorm(n=1,mean= rep(mu0, R), sigma = C_mu), nrow = R, ncol = 1)

# Compute linear predictor and connection probabilities
S <- array(NA, dim = c(V,V,R))
for(t in 1:R){
      S[,,t] <- mu[t] + X[,,t] %*% t(X[,,t])
      diag(S[,,t]) <- NA
}  

P <- phi(S)
dimnames(P) <- list(V.names, V.names, T.names )

# Look at the distribution of connection probabilities
hist(c(P))
round(quantile(c(P), seq(0,1, 0.1), na.rm = TRUE), 4)

### Sample edges
Y <- array(NA, dim = c(V,V,R))
dimnames(Y) <- list(V.names, V.names, T.names )

# Using loop for clarity not efficiency
for(r in 1:R){
  for(i in 2:V){
    for(j in 1:(i-1)){
      Y[i,j,r] <- Y[j,i,r] <- rbinom(1,1,prob = P[i,j,r])
    }
  }
}

### Visualization: start with a random vertex
v <- sample(1:V, 1)
p1 <- pheatmap(P[v,,], cluster_rows = FALSE, cluster_cols = FALSE, 
         color=colorRampPalette(brewer.pal(9,"Blues")[c(1,8)])(30), 
         main = paste0("Probability Matrix: V = ", v))[[4]]

p2 <- pheatmap(Y[v,,], cluster_rows = FALSE, cluster_cols = FALSE, 
         color=colorRampPalette(brewer.pal(9,"Blues")[c(1,8)])(30), 
         main = paste0("Adjacency Matrix: V = ", v))[[4]]
# 
# p3 <- data.frame(LF1 = X[, 1], LF2 = X[,2], id = LETTERS[1:V]) %>%
# ggplot(aes(x = LF1, y = LF2, label = id)) +
#   geom_text() +
#   theme_minimal() +
#   ggtitle("Latent Space") +
#   theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p1, p2, nrow = 1)


```

```{r, echo = FALSE}
# Define model with BUGS code
DfmCode <- nimbleCode({
  
  # Intercept
  mu[1:R] ~ dmnorm(mu0[1:R], cov = C_mu[1:R, 1:R])
  
  # Shrinkage process
  U[1] ~ dgamma(a1, 1)
  for(h in 2:H){
    U[h] ~ dgamma(a2, 1)
  }

  for(h in 1:H){
    Tau[h] <- prod(U[1:h])
  }

  # Latent factors
  for(h in 1:H){
      mvCov[1:R, 1:R, h] <- Tau[h]^(-1)*C_X[,] # Array construction to avoid multiply-defined nodes on LHS
    for(v in 1:V){
        X[v,h,] ~ dmnorm(mean = muX[], cov = mvCov[1:R, 1:R, h]) # no expressions in mvt densities
      }
    }

  # Compute linear predictor and connection probabilities
  for(t in 1:R){
        P[1:V,1:V,t] <- phi(mu[t] + X[,,t] %*% t(X[,,t]))
  }
  
  # Likelihood
  for(t in 1:R){
    for(i in 2:V){
      for(j in 1:(i-1)){
        Y[i,j,t] ~ dbin(size = 1, prob = P[i,j,t])
        #vlogL <- log((P[i,j,t]^Y[i,j,t])*((1 - P[i,j,t])^(1-Y[i,j,t])))
      }
    }
  }

})



# Define the constants
mu0 = log(mean(Y, na.rm = TRUE)/(1-mean(Y, na.rm = TRUE))) # prior mean for Mu is logit(meanY)
for(v in 1:v){
  Y[v,v,] <- 0
}

DfmConsts <- list(V = V,
                 H = H, 
                 R = R,
                 a1 = a1, a2 = a2,
                 mu0 = rep(mu0, R), 
                 C_mu = C_mu, C_X = C_X, # Providing GP parameters as constants
                 muX = rep(0,R))

# Define the data
DfmData <- list(Y = Y)

# Set initialization parameters
DfmInits <- list(X =array(0, dim = c(V,H,R)), 
                U = 1:H,
                mu = rep(mu0, R)) # , array.logL = array(NA, dim = c(V,V,R))

DfmDims <- list(Tau = H, X = c(V, H, R), P = c(V,V,R), mvCov = c(R,R,H)) 
#  S = c(V,V,R), array.logL = c(V,V,R), logL = 1

# Create NIMBLE model
Dfm <- nimbleModel(code = DfmCode, name = "Dfm", constants = DfmConsts, data = DfmData, 
                   dimensions = DfmDims, inits = DfmInits)

# Check conjugacy
configureMCMC(Dfm, print = TRUE)

# Easiest way to run: 
niter <- 5000
nchains <- 1
mcmc.out <- nimbleMCMC(code = DfmCode, constants = DfmConsts,
                       data = DfmData, inits = DfmInits,
                       nchains = nchains, niter = niter,
                       summary = TRUE, WAIC = TRUE,
                       monitors = c('mu', 'Tau', 'P')) 
```



```{r}
# Define R models
HMCmodel <- nimbleModel(code = DfmCode, name = "Dfm", constants = DfmConsts, data = DfmData, 
                   dimensions = DfmDims, inits = DfmInits, buildDerivs = TRUE) # we need to enable derivative capabilities for the model 
HMCmcmc <- buildHMC(HMCmodel, 
                    monitors = c("mu", "X", "Tau", "P")) # if we don't supply node names, all non-data nodes sampled via HMC
# Compile the C models
Cmodel <- compileNimble(HMCmodel)
Cmcmc <- compileNimble(HMCmcmc, project = HMCmodel)

start.time <- Sys.time()
samplesHMC <- runMCMC(Cmcmc, niter = 5000, nchains =1)
Sys.time() - start.time

file_path <- here::here(paste0(save_path, "DfmHMC_probit.rds"))
if(save_files){saveRDS(samplesHMC, file_path)}

samplesHMC <- readRDS(file_path)

# Extract samples and look at them
p.HMC <- samplesHMC[, grepl("P", colnames(samplesHMC))]
thing <- colMeans(p.HMC)
ppost.array <- array(colMeans(p.HMC), dim = c(V,V,R))
for(v in 1:V){
  ppost.array[v,v,] <- NA
}
plot(c(ppost.array), c(P))

# MCMC diagnostics
mcmc_trace(samplesHMC, regex_pars = c("mu")) 

```