---
title: "Latent Factors"
output: html_document
date: "2023-10-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# TO DO:

-   Write out model
-   Does probit work?
-   Vary Tau and try probit via HMC again
-   If not: experiment with k and post question to forum
-   X Simplify Nimble code
-   X Simulate data and code Nimble model
-   Can I get it to work without k?
-   Figure out conjugacy - maybe make that an exercise. 
-   X Add polyga gamma
-   Evaluate performance
-   Add cross validation


```{r, echo = FALSE, message = FALSE}
## Set directories
data_path <- 'Data/'
save_path <- 'Results/'

# Load packages
library(tidyverse)
library(here)
library(igraph)
library(ggraph)
library(RColorBrewer)
library(pheatmap)
library(nimble)
library(nimbleHMC)
library(bayesplot)
library(mvtnorm)
library(gridExtra)

# Do we want to save any output?
save_files <- TRUE

```
# PART 1: SIMULATE FROM A LATENT SPACE MODEL 

First, we simulate data from an inner product graph model. Let's begin with only
10 nodes for ease of visualization

## 1a. Visualizing a small network 
Here we simulate from a latent space network model. Note that although there is no
data augmentation $Z$ in the code below, however it is equivalent the the Albert and Chib (1993)
method presented in lecture. 
```{r}
set.seed(1234)

# Data (and latent space) dimensions
V <- 100 # Number of vertices
H <- 10 # Dimension of latent space

# Hyperparameters
a1 <- 2.5 # Gamma shrinkage parameter for factor 1
a2 <- 2.5 # Gamma shrinkage parameters for factors 2:H

meanP <- 0.25 # Moderately sparse network
mu0 <- probit(meanP) # Prior mean for intercept
sd.mu <- 0.1 # Prior sd for intercept: consider probit scale

# Simulate multiplicative gamma shrinkage process
U <- rep(NA,H)
U[1] <- rgamma(1, a1, 1)
U[2:H] <- rgamma(H-1, a2, 1)

Tau <- rep(NA, H)
for(h in 1:H){
  Tau[h] <- prod(U[1:h])
}

# Simulate latent factors
X <- matrix(NA, nrow = V, ncol = H)
for(h in 1:H){
  X[,h] <- rmvnorm(n = 1, mean = rep(0, V), sigma = diag(Tau[h]^(-1), nrow = V)) 
}

# Simulate intercept
mu <- rnorm(1,mean = mu0, sd = sd.mu) # Normal prior for baseline connection score

# Compute the linear predictor and Z latent connection score
M.Z <- mu + X %*% t(X) 
P <- phi(M.Z) # Gaussian CDF

# Look at the distribution of connection probabilities
hist(c(P))
round(quantile(c(P), seq(0,1, 0.1), na.rm = TRUE), 4)

# Sample edges
Y <- matrix(NA, V,V)
Y[upper.tri(Y)] <- rbinom(V*(V-1)/2, 1, P[upper.tri(P)])
Y[lower.tri(Y)] <- t(Y)[lower.tri(Y)]

diag(Y) <- diag(P) <- NA # Makes plotting more clear

rownames(Y) <- colnames(Y) <- rownames(P) <- colnames(P) <- LETTERS[1:V]

p1 <- pheatmap(P, cluster_rows = FALSE, cluster_cols = FALSE, 
         color=colorRampPalette(brewer.pal(9,"Blues")[c(1,8)])(30), 
         main = "Probability Matrix")[[4]]

p2 <- pheatmap(Y, cluster_rows = FALSE, cluster_cols = FALSE, 
         color=colorRampPalette(brewer.pal(9,"Blues")[c(1,8)])(30), 
         main = "Adjacency Matrix")[[4]]

p3 <- data.frame(LF1 = X[, 1], LF2 = X[,2], id = LETTERS[1:V]) %>% 
ggplot(aes(x = LF1, y = LF2, label = id)) + 
  geom_text() + 
  theme_minimal() + 
  ggtitle("Latent Space") + 
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p1, p2, p3,  nrow = 1)


```
## 1b. Exploring a larger network

Next, we simulate a larger network to which we'll fit our latent space model. 
```{r}
set.seed(1234)

# All hyperparameters and data structures are as above except:
V <- 100 # Number of vertices

# Simulate multiplicative gamma shrinkage process
U <- rep(NA,H)
U[1] <- rgamma(1, a1, 1)
U[2:H] <- rgamma(H-1, a2, 1)

Tau <- rep(NA, H)
for(h in 1:H){
  Tau[h] <- prod(U[1:h])
}

# Simulate latent factors
X <- matrix(NA, nrow = V, ncol = H)
for(h in 1:H){
  X[,h] <- rmvnorm(n = 1, mean = rep(0, V), sigma = diag(Tau[h]^(-1), nrow = V)) 
}

# Simulate intercept
mu <- rnorm(1,mean = mu0, sd = sd.mu) # Normal prior for baseline connection score

# Compute the linear predictor and Z latent connection score
M.Z <- mu + X %*% t(X) 
P <- phi(M.Z)

hist(c(P))
round(quantile(c(P), seq(0,1, 0.1), na.rm = TRUE), 4)

# Sample edges
Y <- matrix(NA, V,V)
Y[upper.tri(Y)] <- rbinom(V*(V-1)/2, 1, P[upper.tri(P)])
Y[lower.tri(Y)] <- t(Y)[lower.tri(Y)]
```

Question: which parameters can you vary? What happens if you change the dimension 
of the latent space? Modify the hyperparameters of the shrinkage process. 
How can you produce a more bimodal vs uniform distribution of latent probabilities?

A. 
First, we vary the latent factor threshold $H$ while fixing all other hyperparameters. As the number of latent factors increases, the majority are concentrated strongly around 0, however given the increasing number of terms in the sum, the range inner products (and hence the linear predictor) increases; the overall effect on edge probabilities is minor increase in the number of extreme probabilities generated. Note that depending on the degree of shrinkage, all factors over a certain threshold will have a negligible effect. In this example, there is relatively little difference between five and ten latent factors, however with less shrinkage we would expect to see a more pronounced difference.  


Next we vary the shrinkage on the latent factors via the parameters of the $Gamma$ distributions, $a_1, a_2$ while fixing the number of latent factors. Increasing the value of these hyperparameters shrinks the prior variance of the latent factors towards 0 such that they concentrate more strongly around their prior mean, 0. When the latent factor threshold is moderate, this has a dramatic effect on the edge probabilities: with $a_1 = a_2 =2$, the distribution of probabilities is nearly uniform and as we increase $a_1, a_2$ further, the distribution becomes concentrated around relatively high probabilities. $a_1, a_2 \approx 1$ result in less shrinkage and hence larger values of the latent factors
and more polarized values of the inner products. 

```{r, echo = FALSE}
# Define model with BUGS code
fmCode <- nimbleCode({
  
  # Intercept
  mu ~ dnorm(mu0, sd = sd.mu)
  
  # Shrinkage process
  U[1] ~ dgamma(a1, 1)
  for(h in 2:H){
    U[h] ~ dgamma(a2, 1)
  }

  for(h in 1:H){
    Tau[h] <- prod(U[1:h])
  }

  # Latent factors
  for(h in 1:H){
    for(v in 1:V){
      X[v,h] ~ dnorm(M[v] , sd = sqrt(Tau[h]^(-1)))
    }
  }
  
  # Compute linear predictor
  M.Z[1:V,1:V] <- mu + X[,] %*% t(X[,]) # Recall multivariate nodes must be used with []

  # Likelihood
  for (i in 2:V){
    for (j in 1:(i-1)){ # Self-edges not allowed
      P[i,j] <- phi(M.Z[i,j]) # Gaussian CDF 
      P[j,i] <- P[i,j]
      Y[i,j] ~ dbin(size = 1, prob = P[i,j])
    }
  }

})



# Define the constants
diag(Y) <- 0 # diag = NA is useful for plotting only
mu0 = log(mean(Y)/(1-mean(Y))) # prior mean for Mu is logit(meanY)

fmConsts <- list(V = V,
                 H = H, 
                  a1 = a1, a2 = a2,
                  mu0 = mu0, sd.mu = sd.mu,
                  M = rep(0, V))

# Define the data
fmData <- list(Y = Y)

# Set initialization parameters
fmInits <- list(X = matrix(0, V, H), 
                U = 1:H,
                mu = mu0)

fmDims <- list(Tau = H, X = c(V, H), P = c(V,V))

# Create NIMBLE model
fm <- nimbleModel(code = fmCode, name = "fm", constants = fmConsts, data = fmData, 
                   dimensions = fmDims, inits = fmInits)

# Check conjugacy
configureMCMC(fm, print = TRUE)

# Easiest way to run: 
niter <- 5000
nchains <- 2
mcmc.out <- nimbleMCMC(code = fmCode, constants = fmConsts,
                       data = fmData, inits = fmInits,
                       nchains = nchains, niter = niter,
                       summary = TRUE, WAIC = TRUE,
                       monitors = c('mu', 'Tau', 'P')) 
```
Evaluate performance

```{r}
head(mcmc.out$summary)
fm.samples <- do.call(rbind, mcmc.out$samples)
p.samples <- fm.samples[, grepl("P", colnames(fm.samples))]

p.post <- matrix(data = colMeans(p.samples), byrow = FALSE, nrow = V, ncol = V)
diag(p.post) <- 0
plot(c(p.post), c(P))


# Not really useful for a single model, but: 
mcmc.out$WAIC

## MCMC diagnostics: unclear how useful because of label switching
fm.samples2 <- mcmc.out$samples

mcmc_trace(fm.samples2, regex_pars = c("mu")) 
mcmc_trace(fm.samples2, regex_pars = c("X")) 
mcmc_trace(fm.samples2, regex_pars = c("Tau")) 

mcmc_intervals(fm.samples2, regex_pars = c("X"))
mcmc_intervals(fm.samples2, regex_pars = c("Tau"))
mcmc_intervals(fm.samples2, pars = paste0("Tau[", 1:5, "]"))

```

Debugging: Tau and X demonstrate appropriate shrinkage at least, Mu is not moving far enough from prior mean,
latent factors aren't able to capture graph structure. So, this is terrible. Let's see if HMC does any better. 

```{r}
# Define R models
HMCmodel <- nimbleModel(code = fmCode, name = "fm", constants = fmConsts, data = fmData, 
                   dimensions = fmDims, inits = fmInits, buildDerivs = TRUE) # we need to enable derivative capabilities for the model 
HMCmcmc <- buildHMC(HMCmodel, 
                    monitors = c("mu", "X", "Tau", "P")) # if we don't supply node names, all non-data nodes sampled via HMC
# Compile the C models
Cmodel <- compileNimble(HMCmodel)
Cmcmc <- compileNimble(HMCmcmc, project = HMCmodel)

start.time <- Sys.time()
samplesHMC <- runMCMC(Cmcmc)
Sys.time() - start.time

file_path <- here::here(paste0(save_path, "fmHMC_probit.rds"))
if(save_files){saveRDS(samplesHMC, file_path)}

#samplesHMC <- readRDS(file_path)

# Extract samples and look at them
p.HMC <- samplesHMC[, grepl("P", colnames(samplesHMC))]

p.post.HMC <- matrix(data = colMeans(p.HMC), byrow = FALSE, nrow = V, ncol = V)
diag(p.post.HMC) <- 0
plot(c(p.post.HMC), c(P))
```