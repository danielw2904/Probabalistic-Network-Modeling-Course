---
title: "DC-SBM"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = "..")
```

```{r echo = FALSE, message = FALSE}

## Packages

# One package will require install from github
if("mcclust.ext" %in% rownames(installed.packages()) == FALSE) 
  {devtools::install_github('sarawade/mcclust.ext')}
library(mcclust.ext)

# The rest are straightforward
library(nimble)
library(nimbleHMC)
library(pheatmap)
library(RColorBrewer)
library(tidyverse)
library(gridExtra)
library(here)
library(bayesplot)
library(truncnorm)
library(pROC)
library(latex2exp)

## Set directories
data_path <- 'Data/'

```


# 1. Simulate and fit a DC-SBM

## 1a. Simulate from a degree-corrected SBM

The degree-corrected SBM permits additional heterogeneity in degree
distribution compared to the basic SBM - much like the random effects
network logistic regression we worked with previously. However, whereas
the logistic regression was based on a known clustering via shared
region, the SBM of course learns the clustering. Our DC-SBM takes the
following form:

$\pi_{ij}=g^{-1}(\mu_{Z_i,Z_j}+\alpha_i + \alpha_j)$

We assume independent Dirichlet-multinomial priors on group membership
as before:

$$\pi(Z \mid \lambda ) = \prod_{p=1}^n Z_{p.}^T \lambda \\
    \lambda \sim \text{Dirichlet}(\alpha 1_K) \\$$

Note that while we model the block connection probabilities with a Beta
distribution in the basisc SBM, in this specificaiton of the DC-SBM the
link function $g$ facilitates the use of normal priors for the block
connection and degree correction terms.

$$\mu_{hl} \sim  N(m_{1(h=l)},v_{1(h=l)}) \\
\alpha_i \sim N(0,\tau)\\
\tau^{-1/2}\sim N_+(0,1)\\$$

Now we simulate from the model. 

```{r}
set.seed(1)

# Setup data structure
V <- 25
K <- 5

# Set hyperparameters
sd.tau <- 1 # degree correction sd: determines amount of heterogeneity
m0 <- log(0.25/(1-0.25)) # low between block connectivity
v0 <- 1
m1 <- log(0.75/(1-0.75)) # high within block connectivity
v1 <- 1

# Simulate group membership
lambda <- rdirch(n=1, alpha = rep(5/K, K)) 
z <- sample(1:K, size = V, prob = lambda, replace = TRUE)
z.mat <- matrix(0, nrow = V, ncol = K)
for(i in 1:V){
  z.mat[i,z[i]] <- 1
}

# Simulate within and between community connection probabilities
mu <- matrix(NA, nrow = K, ncol = K) # matrix of group-edge probabilities
mu[upper.tri(mu, diag = FALSE)] <- rnorm(n = K*(K- 1)/2, mean = m0, sd = sqrt(v0)) # Between communities
mu[lower.tri(mu, diag = FALSE)] <- t(mu)[lower.tri(mu, diag = FALSE)]
diag(mu) <- rnorm(K, mean = m1, sd = sqrt(v1)) # Within community

# Simulate random effects variance and random effects (degree correction terms)
tau.star <- rtruncnorm(1, a = 0, b = Inf, mean = 0, sd = sd.tau)
tau <- tau.star^(2) # Reasonable RE variance, we'll keep it
alpha <- rnorm(V, mean = 0, sd = tau)
alpha.mat <- outer(alpha, alpha, FUN = "+")
diag(alpha.mat) <- NA # no self edges

# Compute connection score for each pair of nodes
s.mat <- z.mat %*% mu %*% t(z.mat) + alpha.mat

# Get probabilities and examine distribution
pi.mat <- 1/(1 + exp(-s.mat))
hist(c(pi.mat))

# Simulate data
Y <- matrix(NA, nrow = V, ncol = V)
Y[upper.tri(Y)] <- rbinom(V*(V - 1)/2, 1, pi.mat[upper.tri(pi.mat)])
Y[lower.tri(Y)] <- t(Y)[lower.tri(Y)]
rownames(Y) <- colnames(Y) <- paste0("Node ", 1:V)

# Might need to add diag to Y to avoid errors in Nimble
diag(Y) <- 0

# Order the nodes by their community labels
sel <- order(z)
z_sorted <- z[sel]
Y_sorted <- Y[sel, sel]

# Plot the adjacency matrix using a heatmap
pheatmap(Y_sorted, cluster_rows = F, cluster_cols = F,
         color=colorRampPalette(brewer.pal(9,"Greys")[c(1,8)])(30), 
         main = "Observed Adjacency Matrix", 
         fontsize = 8, show_rownames = T, show_colnames = T, 
         legend = F)

```

Plotting the adjacency matrix, we see that the data is moderately assortative, 
however this trend is somewhat obscured by heterogeneity in vertex degree. Note
that while some vertices connect only within their communities, other vertices
connect more widely. 

------------------------------------------------------------------------

### EXERCISE

1.  Rerun the simulation code above, starting with a new seed each time.
    Comment on the impact of $\tau$ on network structure.

------------------------------------------------------------------------

## 1b. Create a DC-SBM with NIMBLE

Now we define and fit the model with Nimble. Note that I am using the
probit link function, $g^{-1} = \Phi$, the cumulative distribution
function of the standard normal distribution.

------------------------------------------------------------------------

### EXERCISE

2.  Partial code defining the Nimble DC-SBM is provided below. Complete
    it by adding the chunks defining the priors for $\mu$ and $\alpha$,
    and add the additional an additional computed quantity for the log
    likelihood (useful for checking mixing).

------------------------------------------------------------------------

```{r, eval = FALSE, echo = FALSE}

# Partial code
DCsbmCode <- nimbleCode({
  
  # lambda - latent group assignment probabilities (vector of length K)
  lambda[1:K] ~ ddirch(alpha.dirch[]) 
  
  # Z - latent group indicator (binary vector of length K, summing to 1)
  for(i in 1:V){
    Z[i] ~ dcat(prob = lambda[])
  }

  # Mu - symmetric matrix of within and between group edge probabilities

  
  # Tau: RE variance
  tau.star ~ T(dnorm(mean = 0, sd = sd.tau), 0, ) # note declaration of trunc density
  tau <- tau.star^(-2) 
  
  # Alpha: degree correction terms


  # Pi - node to node edge probabilities (based on group membership)
  for (i in 2:V){
    for (j in 1:(i-1)){ # Self-edges not allowed
      S[i,j] <- myCalculation(Mu[,], Z[i], Z[j]) + Alpha[i] + Alpha[j]
      S[j,i] <- S[i,j]
      Pi[i,j] <- 1/(1+e^(-S[i,j]))
      Pi[j,i] <- Pi[i,j]
      Y[i,j] ~ dbin(size = 1, prob = Pi[i,j])
    }
  }
  
  # Add zero diagonal to avoid NA in post samples error (we don't actually use these)
  for(i in 1:V){ 
    S[i,i] <- 0
    Pi[i,i] <- 0
  }
  
  # Compute logL
  

})

## User-defined functions: written in NIMBLE
myCalculation <- nimbleFunction(
  run = function(grid = double(2), index1 = double(0), index2 = double(0)) {  
    return(grid[index1, index2])
    returnType(double(0))
  })
```

*Answer*

```{r, cache = TRUE, echo = TRUE}
# Define model with BUGS code
DCsbmCode <- nimbleCode({
  
  # lambda - latent group assignment probabilties (vector of length K)
  lambda[1:K] ~ ddirch(alpha.dirch[]) 
  
  # Z - latent group indicator (binary vector of length K, summing to 1)
  for(i in 1:V){
    Z[i] ~ dcat(prob = lambda[])
  }

  # Mu - symmetric matrix of within and between group edge probabilities
  for (i in 1:K){
    Mu[i,i] ~ dnorm(mean = m1, sd = sqrt(v1)) # Within block connections
    for (j in 1:(i-1)){ 
      Mu[i,j] ~ dnorm(mean = m0, sd = sqrt(v0)) # Between block connections 
      Mu[j,i] <- Mu[i,j] # symmetric matrix
    }
  }
  
  # Tau: RE variance
  tau.star ~ T(dnorm(mean = 0, sd = sd.tau), 0, ) # note declaration of trunc density
  tau <- tau.star^2 
  for(v in 1:V){
    Alpha[v] ~ dnorm(mean = 0, sd = tau)    
  }


  # Pi - node to node edge probabilities (based on group membership)
  for (i in 2:V){
    for (j in 1:(i-1)){ # Self-edges not allowed
      S[i,j] <- myCalculation(Mu[,], Z[i], Z[j]) + Alpha[i] + Alpha[j]
      S[j,i] <- S[i,j]
      #Pi[i,j] <- phi(S[i,j]) # Gaussian CDF (inverse probit)
      Pi[i,j] <- 1/(1+exp(-S[i,j]))
      Pi[j,i] <- Pi[i,j]
      Y[i,j] ~ dbin(size = 1, prob = Pi[i,j])
    }
  }
  
  # Add zero diagonal to avoid NA in post samples error (we don't actually use these)
  for(i in 1:V){ 
    S[i,i] <- 0
    Pi[i,i] <- 0
  }
  
  # Compute logL
  for (i in 2:V){
    for (j in 1:(i-1)){ # Self-edges not allowed
        mat.logL[i,j] <- log((Pi[i,j]^Y[i,j])*((1 - Pi[i,j])^(1-Y[i,j])))
        #mat.logL[j,i] <- 0
    }}
  
  logL <- sum(mat.logL[1:V, 1:V])/2 # diag is zero so this works

})

## User-defined functions: written in NIMBLE
myCalculation <- nimbleFunction(
  run = function(grid = double(2), index1 = double(0), index2 = double(0)) {  
    return(grid[index1, index2])
    returnType(double(0))
  })


```

Next, we define, the data, constants, dimensions, and initializations
and run the model.

```{r}

# Define the constants
DCsbmConsts <- list(V = V, K= K,
                  m0 = m0, v0 = v0, m1 = m1, v1 = v1,
                  sd.tau = sd.tau, 
                  alpha.dirch =  rep(1/K, K))

# Define the data
DCsbmData <- list(Y = Y)

# Set initialization parameters
DCsbmInits <- list(lambda = rep(1/K, K), # block assignment probs
                 Mu = matrix(probit(mean(Y)), K, K), # block connection probs
                 tau.star = 1, tau = 1, # RE variance parameters
                 Alpha = rep(probit(mean(Y)), V), # random effects
                 Pi = matrix(mean(Y), V, V), # pairwise edge probs
                 S = matrix(probit(mean(Y)), V, V), # extra var for debugging
                 Z = sample(1:K, size = V, replace = TRUE), 
                 mat.logL = matrix(0,V,V))

DCsbmDims <- list(lambda = c(K), theta = c(K, K), Pi = c(V, V), S = c(V,V), mat.logL = c(V,V))

# Create NIMBLE model
DCsbm <- nimbleModel(code = DCsbmCode, name = "DCsbm", constants = DCsbmConsts, data = DCsbmData, 
                   dimensions = DCsbmDims, inits = DCsbmInits)
# Check conjugacy
configureMCMC(DCsbm, print = TRUE)

# Easiest way to run: 
niter <- 10000
nchains <- 2
mcmc.out <- nimbleMCMC(code = DCsbmCode, constants = DCsbmConsts,
                       data = DCsbmData, inits = DCsbmInits,
                       nchains = nchains, niter = niter,
                       summary = TRUE, WAIC = TRUE,
                       monitors = c('lambda', 'Pi', 'Mu', 'Alpha', 'tau', 'logL', 'Z'))
```

## 1c. MCMC Diagnostics

```{r, echo = FALSE}

# Extract samples: list with items corresponding to chains
DCsbm.samples <- mcmc.out$samples

# Extract all samples and bind together
samples <- do.call(rbind, mcmc.out$samples)
pi.samples <- samples[, grepl("Pi", colnames(samples))]
alpha.samples <- samples[, grepl("Alpha", colnames(samples))]

# Log likelihood
mcmc_trace(DCsbm.samples, regex_pars = c("logL")) 

# Block connection probabilities
mcmc_trace(DCsbm.samples, regex_pars = c("Mu")) 

# Block assignment probabilities
mcmc_trace(DCsbm.samples, regex_pars = c("lambda")) 

```

## 1d. Evaluate parameter recovery and accuracy

First, to get an overview of our model's performance, we take advantage
of the fact that we have simulated data and evaluate the recovery of
latent edge probabilities across all pairs of nodes. In the code below,
we plot true latent probabilities against the posterior mean
probabilities, coloring each point to indicate if it corresponds to an
edge between nodes in the same cluster.

```{r}
# Extract posterior connection probabilities
p.post <- matrix(data = colMeans(pi.samples), byrow = FALSE, nrow = V, ncol = V)
diag(p.post) <- NA

# Plotting setup
samegroup.mat <- z.mat %*% t(z.mat) # define common group indicator
df <- data.frame(pred = p.post[upper.tri(p.post)], p.true = pi.mat[upper.tri(pi.mat)], 
                 y = Y[upper.tri(Y)],samegroup = as.factor(samegroup.mat[upper.tri(samegroup.mat)]))
ggplot(df, aes(x = pred, y = p.true, color = samegroup)) + 
                                    geom_point(aes(alpha = I(0.3))) + # replace with geom_jitter if probs v similar
                                    geom_abline(intercept = 0, slope = 1, color = "gray") + 
                                    xlab(parse(text = TeX('$\\hat{\\pi}_{ij}$'))) + 
                                    ylab(parse(text = TeX('$\\pi_{ij}$'))) + 
                                    scale_color_viridis_d(name = "", labels = c("Different clusters", "Same cluster")) + 
                                    ggtitle("DC-SBM Accuracy on Simulated Data") +
                                    theme_minimal() + 
                                    theme(text = element_text(family = "serif", size = 14))


plot.roc(df$y ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - DC-SBM")

```

------------------------------------------------------------------------

### Exercise

Examine the recovery posterior samples of the random effect parameters
$\alpha$ and the block connection parameters $\mu$. Is label-switching a
concern for either of these? Comment on the relative importance of
community and node-level effects in determining connection probabilities
in this particular sample.

------------------------------------------------------------------------


------------------------------------------------------------------------

### EXERCISE

Vary the value of $\tau$ to increase heterogeneity. How does this impact
overall accuracy and recovery of the random effect parameters $\alpha$
and the between block connection parameters $\mu$.

------------------------------------------------------------------------

#2. Trade application

We continue our analysis of the 2021 OECD trade network. Begin with
they same hyperparameters as above, but feel free to modify
(particularly the structure of the $\mu$, i.e., the level of baseline
assortativeness), and fit the DC-SBM to the trade data. 


------------------------------------------------------------------------

# ASSIGNMENT

------------------------------------------------------------------------

Continue working with your selected data set and analyze it using the
degree-corrected stochastic block model. Vary your priors, particularly
the degree of assortativeness and compare the performance of the
resultant models with respect to AUC, WAIC, etc.

Some additional questions you might consider:

-   How does the clustering produced by DC-SBM compare to the clustering
    produced by the classical SBM, and any natural groupings in the
    data? Can you interpret the clustering based on your knowledge of
    the vertices?

-   How does the DC-SBM perform with respect to WAIC and accuracy
    compared to the other models you have analyzed? If it performs
    better than the network logistic GLMM, comment on any potential
    evidence for residual clustering of connectivity patterns beyond
    that captured by region and vertex degree.
