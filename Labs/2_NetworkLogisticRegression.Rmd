---
title: "Network Logistic Regression Lab"
output: html_document
date: "2023-10-06"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Housekeeping

```{r, echo = FALSE, message = FALSE}
## Set directories
data_path <- 'Data/'
save_path <- 'Data/'

# Load packages
library(tidyverse)
library(here)
library(igraph)
library(ggraph)
library(RColorBrewer)
library(pheatmap)
library(nimble)
library(nimbleHMC)
library(bayesplot)
library(pROC)
library(caret)
library(coda)
```


For this example, we'll be continuing with the OECD 2021 trade data. We
begin by reading the data and extracting key data structures to use
later on. 

```{r}

# Read the data 
A <- read.csv(here::here(paste0(data_path, "2021Trade/A_subset.csv")))
features <- read.csv(here::here(paste0(data_path, "2021Trade/Features_subset.csv")))

# Extract useful dimensions
nC <- nrow(A)

# Do we want to save any output?
save_files <- TRUE

```


# Introduction to NIMBLE using R

Instructions and a basic tutorial for installing the package `nimble`
are provided in Lab 0: Review R, and the full user manual is available
[here](https://r-nimble.org/manuals/NimbleUserManual.pdf). We'll go
through a brief intro here.

NIMBLE is a powerful system for fitting statistical models in
$\texttt{R}$. The NIMBLE algorithm library includes slice, adaptive
random walk, adaptive block random walk, elliptical slice, Gibbs
samplers, and many more. A predefined list of conjugacy relationships
which Nimble detects is available
[here](https://github.com/nimble-dev/nimble/blob/devel/packages/nimble/R/MCMC_conjugacy.R).

More information on the NIMBLE samplers is available [here](https://rdrr.io/cran/nimble/man/samplers.html).

A NIMBLE model can be thought of as a directed acyclic graph or DAG with
deterministic relations indicated by `<-` and stochastic relationships
indicated by a `~`. Visualizing the DAG for your model is a good way to
ensure that your model is properly declared. Additionally, NIMBLE is a
declarative language and the order of lines in a NIMBLE model does not
matter.

## NIMBLE workflow

The basic NIMBLE workflow consists of the following steps:

1.  Define a model consisting of likelihood and priors (BUGS)

2.  Define your data, initial parameter values, and indicate the
    parameters which you'll be doing inference on (R)

3.  Configure the MCMC by specifying the number of chains, and the
    number of burn-in and post-burn-in iterations (R code)

4.  Compile and run the MCMC (C++)

5.  Extract your results, perform MCMC diagnostics, and perform
    inference on parameters of interest (R)

## NIMBLE tips

Beware of the following quirks:

-   NIMBLE does not allow multivariate nodes to be used without square
    brackets

    -   Vector `x`: use `mean(x[ ])`

    -   Matrix `x`: use `mean(x[ , ])`

-   NIMBLE *does not permit stochastic indexing*: any variables used for
    indexing must be provided as a constant, be a looping index, or
    something deterministically derived from these (i.e., no indexing by
    latent variables or indices that are not constant). If you need to
    index on a latent variable, create a user-provided function to call
    within the model

-   *Multivariate distributions do not take expressions as arguments*:
    all arguments should be given a deterministic definition prior to
    the distribution call

-   *Initialize all parameters* to be safe: failing to do so can cause
    slow convergence or MCMC failure. Tips on initializing your MCMC are
    provided in Chapter 7.4 of the NIMBLE user manual.

-   *Provide dimensions, particularly for multivariate arrays* using the
    `dimensions` argument to the function `nimbleModel` can help
    identify and prevent dimension-related errors
    
-   *Note on symmetric matrices*: we are only intrested in the upper (or lower)
    triangle of our adjacency or probabiilty matrices, and so it is of no concern
    to have NA's or nonsensical 0's in the unused portions. However, this can cause
    problems in NIMBLE for monitored parameters, and so NIMBLE is happier if you 
    assign values to all components of stored matrices. 

# Network logistic regression using NIMBLE

To create our model, the first step is to define the likelihood and prior using BUGS code. 
We will begin with the simple network logistic regression defined in lecture where edges are 
defined as conditionally independent probabilities given by: 
$$\text{logit} ((p(y_{ij}= 1)) = \text{logit} (\pi_{ij}) = \beta_0 + \beta_1 x_{ij},$$ where our edge covariate is an
indicator that the two countries are in the same region, $x_{ij} = 1(R_i = R_j)$.

## Define and run a NIMBLE model 
### Define the model 
```{r, cache = TRUE}
# Define the model 
glmCode <- nimbleCode({
  # Priors
  beta0 ~ dnorm(0, sd = 1) # some use sd = 10000!
  beta1 ~ dnorm(0, sd = 1)
  
  # Likelihood
  for (i in 2:N) { 
    for (j in 1:(i-1)){
      logit(p[i,j]) <- beta0 + beta1 * x[i,j] 
      p[j,i] <- p[i,j]
      y[i,j] ~ dbin(size = 1, prob = p[i,j])
    }
  }
  
  for(i in 1:nC){ # clunky code to avoid NA in p matrix
    p[i,i] <- 0
  }
  
})
```
Note that in the code above, we create a probability matrix. This isn't strictly 
necessary, but it will make interpreting our model results more straightforward, 
particularly for more complex models. 

### Provide data, constants, initial values
Now we provide the data, constants, and initial values to perform the MCMC. 

```{r}
# Define the model constants
glmConsts <- list(N = nC)

# Define the model data
reg <- as.factor(features$region) 
glmData <- list(
  y = A,
  x = outer(reg,reg, FUN = "==")*1 # x_ij = 1(region_i == region_j)
)

# Define the initial values
glmInits <- list(beta0 = 0, beta1 = 0, p = matrix(0, nC, nC))

# Define the dimensions
glmDims <- list(p = c(nC, nC))
```
### Create a NIMBLE model object and check conjugacy
Next, we process our BUGS code, constants, data, and initial values into a NIMBLE model. 

```{r}
# Create the Nimble model object
glmModel <- nimbleModel(code = glmCode, constants = glmConsts, data = glmData, 
                         inits = glmInits)

```

We can also configure the MCMC and check conjugacy; for now we're leaving the default 
configuration as is, but you can pass additional arguments to configureMCMC if desired.
At this point, we can also look at the DAG. For our model, there are too many nodes 
to effectively visualize, but you can always subset your data and run `glmModel$plotGraph()`
to make sure the DAG is as expected. 

```{r}
# Check conjugacy
configureMCMC(glmModel, print = TRUE)
```
The above show that NIMBLE has not detected conjugacy and so the default
sampler is an adaptive Random Walk Metropolis-Hastings sampler; this sampler
uses the adaptation scheme of Shaby and Wells (2011). 


### Fit the Model 
The fastest way to fit our model is using the one-line MCMC invocation `nimbleMCMC`.
```{r, cache = TRUE, message = FALSE}
# MCMC invocation
set.seed(1234)
niter <- 2000
nchains <- 2
mcmc.out <- nimbleMCMC(code = glmCode, constants = glmConsts,
                       data = glmData, inits = glmInits,
                       nchains = nchains, niter = niter,
                       summary = TRUE, WAIC = TRUE,
                       monitors = c('beta0', 'beta1', 'p'), 
                       samplesAsCodaMCMC = TRUE) # Use this option if you plan on using coda

```

## MCMC Diagnostics


MCMC diagnostics are tools for determining if the MCMC samples provide and accurate
approximation of our target posterior distribution, i.e. whether the chains have 
converged. For a quick primer on visual MCMC diagnostics with the package `bayesplot`, visit
the tutorial [here](https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html).

### Prior predictive checks
Before we dive into our MCMC samples, we perform a prior predictive check. 
In order to determine whether a prior is reasonable, we can generate data from the 
prior and compare it to our observed data. 

We simulate from the prior in the code below. Note that
we are NOT using the data in the adjacency matrix $A$ to compute our prior predictive distribution, 
we are using the data structure, i.e. the extent of regional overlap, stored in the matrix $X$. 

In the same manner, we can use our posterior samples to perform posterior predictive checks. 


```{r}
# Vectorize our data: we'll compare this to our prior predictive samples
obsA <-  A[upper.tri(A)]

# Set up for the prior predictive
n.samples <- 100
x <- outer(reg,reg, FUN = "==")*1
p.pp <- y.pp <- array(data = NA, dim = c(nC, nC, n.samples))

# Simulate from the prior
for(r in 1:n.samples){
  # Priors
  beta0 <- rnorm(1, 0, sd = 1) # some use sd = 10000!
  beta1 <- rnorm(1, 0, sd = 1)
  
  # Model
  for (i in 2:nC) { 
    for (j in 1:(i-1)){
      p.pp[i,j,r] <- p.pp[j,i,r] <- ilogit(beta0 + beta1 * x[i,j] )
      y.pp[i,j,r] <- rbinom(n = 1, size = 1, prob = p.pp[i,j, r])
    }
  }
  
}

p.psamples <- t(apply(p.pp, 3, function(x) x[lower.tri(x)]))
y.psamples <- t(apply(y.pp, 3, function(x) x[lower.tri(x)]))

pp_check(obsA, y.psamples[1:50, ], ppc_dens_overlay)
pp_check(obsA, y.psamples[1:8, ], ppc_hist)

```

Which of the two plots above makes more sense for our data? 
Does this look like a reasonable model for our data? 

If the prior seems satisfactory based on the analysis above, we can move onto analyzing our posterior samples. 
First, we do some light processing to get our samples into a more convenient format

```{r}

# Extract samples 
samples <- mcmc.out$samples # as list of chains
all.samples <- do.call(rbind, mcmc.out$samples) # as matrix

# Extract posterior samples of the probability matrix
p.samples <- all.samples[, grepl("p", colnames(all.samples))]
p.post <- matrix(data = colMeans(p.samples), byrow = FALSE, nrow = nC, ncol = nC)

```


### Trace plots

One very useful diagnostic plot is the trace plot, a time series plot of the posterior samples from our Markov Chains, this allows us to evaluate the 
evolution of the parameter over time (i.e., ``mixing''). In general, a good trace plot should show
random scatter around a mean; if in contrast we see snaking or local trending, it suggests that
our chains have not mixed well and may not yet have converged to the stationary distribution. 
Mixing looks satisfactory in the plots below. 

```{r}
mcmc_trace(samples, regex_pars = c("beta"))
```


### ACF
ACF plots show the autocorrelation between successive iterations of a given parameter. 
In the ideal case, our MCMC samples would be independent draws from the posterior, and
so we would like to see ACF dropping quickly to 0 as the lag increases. 

We have strong autocorrelation for our paramters, however
they do eventually fall off. 

```{r}
mcmc_acf(samples, regex_pars = c("beta"))
```


### Pairs plots

The function `mcmc_pairs` allows us to look at multiple parameters at once, and is
useful for identifying collinearity between variables. Note that the related function
`mcmc_parcoord` works better with a high number of parameters. 

```{r}
mcmc_pairs(samples, regex_pars = c("beta"))
```

The narrow bivariate scatter for $(\beta_0, \beta_1)$ indicates some collinearity. Is this expected?


### Posterior Summaries
A nice way to summarize the posterior samples and credible intervals is with the 
`mcmc_intervals` function: 

```{r}
mcmc_intervals(all.samples, regex_pars = c("beta"))
head(mcmc.out$summary[[1]])
```

### Interpreting logistic regresion coefficients

Recall the model is $\log(\frac{\pi_{ij}}{1-\pi_{ij}}) = \beta_0 + \beta_1x_{ij}$. Because interpreting coefficients
in terms of the log odds is somewhat awkward, we can instead consider the odds: 

$$
\begin{aligned}
\log(\frac{\pi_{ij}}{1-\pi_{ij}}) &= \beta_0 + \beta_1x_{ij} \\
\implies \frac{\pi_{ij}}{1-\pi_{ij}}&=\exp(\beta_0)\exp(\beta_1x_{ij})
\end{aligned}
$$

This leads to our first interpretation: $\exp(\beta_0)$ gives the baseline odds of a connection for countries in different regions and $\exp(\beta_1)$ gives the multiplicative increase in odds for countries in the same region. 

```{r}

# Extract coefficients
b0.hat <- mean(all.samples[, grepl("beta0", colnames(all.samples))])
b1.hat <- mean(all.samples[, grepl("beta1", colnames(all.samples))])

# Baseline odds
exp(b0.hat) 

# Same region: multiplicative increase in odds
exp(b1.hat)

```

This interpretation is still somewhat awkward however, so we can further transform to obtain: 

$$\pi_{ij} = \frac{1}{1+\exp(-(\beta_0 + \beta_1 x_{ij})} = \text{logit}^{-1}(\beta_0 + \beta_1 x_{ij})$$


This yields an alternative interpretation: $\text{logit}^{-1}(\beta_0)$ gives the baseline probability of a connection for countries in different regions and $\text{logit}^{-1}(\beta_0 + \beta_1)$ gives the connection probability for countries in the same region. 

***

### Exercise

Recall that we can transform our MCMC samples to obtain functionals of our paramters. Transform 
the samples of $\beta_0$ and $\beta_1$ to obtain more interpretable posterior means and functionals. 

***



```{r}

# Baseline connection prob
exp(b0.hat)/( 1 + exp(b0.hat))

# Connection prob for same region
exp(b0.hat + b1.hat)/(1 + exp(b0.hat + b1.hat))

```

## Accuracy

The ROC curve (receiver operating characteristic curve) shows the performance of our
posterior samples as predictors at varying thresholds. Area under the ROC curve (AUC)
provides a summary of performance across all classification thresholds. How does our
very simple model perform?

```{r}
df <- data.frame(pred = p.post[upper.tri(p.post)], data = obsA)
plot.roc(df$data ~ df$pred, percent = TRUE, print.auc = TRUE, main = "ROC Curve - Baseline GLM")

```

One final metric for assessing model fit is the *WAIC* or the Widely Applicable 
Information Cirterion/Watanabe-Akaike Information Criterion(Watanabe, 2010). The WAIC
provides and estimator of prediction error for comparing a set of statistical 
models fit to the same data; lower WAIC scores indicate better fit (think of it 
as the information lost the model relative to the full data). 

In brief,the WAIC is the difference between the *log pointwise predictive density (lppd)* and the 
effective number of parameters $p_{WAIC}$: 

$$\text{WAIC} = -2(lppd -p_{WAIC})$$

A nice explanation
of the various predictive information criteria for Bayesian models is [here](http://www.stat.columbia.edu/~gelman/research/published/waic_understand3.pdf).

NIMBLE computes the WAIC for fitted models, the default option being the conditional WAIC. 
Note that NIMBLE's computation does depend on which nodes are treated as parameters, 
so this should be consistent across models being compared. 
See Chapter 7.8 of the NIMBLE manual for details. 

We extract the WAIC below. While it is not useful in isolation, we will use it for
comparing this first model to subsequent models. 

```{r}
mcmc.out$WAIC
```

## Posterior Predictive Checks

Does our model generate networks the appropriate density?
```{r}
# Compute functionals of parameters for each MCMC iteration

A.r <- matrix(NA, nrow = nC, ncol = nC)
postD <- matrix(NA, nrow = nrow(p.samples), ncol = 1)
allD <- matrix(NA, nrow = nrow(p.samples), ncol = nC)
for(r in 1:nrow(p.samples)){
  
  # Sample adjacency matrix
  p.post.r <- matrix(data = p.samples[r,], byrow = FALSE, nrow = nC, ncol = nC)
  diag(p.post.r) <- 0
  A.r[upper.tri(A.r)] <- rbinom(nC*(nC - 1)/2, 1, p.post.r[upper.tri(p.post.r)])
  A.r[lower.tri(A.r)] <- t(A.r)[lower.tri(A.r)]
  diag(A.r) <- 0
  
  # Sample network density
  postD[r,] <- sum(A.r[lower.tri(A.r)])/(nC*(nC -1)/2)
  allD[r,] <- rowSums(A.r)
}

D.obs <- sum(A[lower.tri(A)])/(nC*(nC -1)/2)
hist(postD)
abline(v = D.obs)

```

Overall network density is well described. 

```{r}
all.D.obs <- rowSums(A)
pp_check(all.D.obs, allD[2000:2050, ], ppc_dens_overlay)
```


Degree distribution is not well characterized by the model. 


# Random effects in the RW sampler

The model above is incredibly inflexible: we describe the entire network with only
two probabilities. To permit greater heterogeneity in connectivity patterns, we could
introduce a random effect for each country. 

$$\text{logit} ((p(y_{ij}= 1)) = \text{logit} (\pi_{ij}) = \beta_1 x_{ij} + \beta_{2i} + \beta_{2j}$$ 

*** 
### Exercise

Implement the random effects model in NIMBLE. Perform MCMC diagnostics and compare to the baseline model.
How is the mixing?

Perform posterior predictive checks. Is the degree distribution captured better?
***




***

# ASSIGNMENT
Continue working with your selected data set and fit one of the network regressions
above. Can you define any edge covariates? Is there a hierarchical structure you
could consider when defining random effects? Feel free to experiment with different
samplers in Nimble (e.g. adaptive block Random Walk using the argument `autoblock = TRUE` in
your configureMCMC call). Consider global-local shrinkage priors if your data has hubs. Additionally, if you are familiar with it, you may choose to include data splitting in your validation approach. 

***



